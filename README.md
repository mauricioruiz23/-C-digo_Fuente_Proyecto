Repositorio de Código Fuente del proyecto en GitHub. 
Instrucciones de ejecución:

1.Implementación del conjunto de datos en Spark.
   -Ejecutar el código nano batch_processing.py
2.Realizar operaciones de limpieza, transformación y análisis exploratorio de datos (EDA), utilizando RDDs o DataFrames. Almacenando los resultados procesados.
   -Ejecutar el código eda_spark.py
3.Implementar la aplicación Spark Streaming en un conjunto de datos que consume del tema de Kafka. Utilice el script en Python usando PySpark para compartir los datos de Kafka.
   -Ejecutar el código spark_kafka_streaming.py
4.Procesar los datos en tiempo real verificando los datos y visualizando los resultados del procesamiento.
   -Ejecutar el código procesamiento_streaming.py
